Node IP: 10.1.20.242
| distributed init (rank 0): env://
| distributed init (rank 2): env://
| distributed init (rank 3): env://
| distributed init (rank 4): env://
| distributed init (rank 10): env://
| distributed init (rank 5): env://
| distributed init (rank 7): env://
| distributed init (rank 9): env://
| distributed init (rank 6): env://
| distributed init (rank 8): env://
| distributed init (rank 11): env://
| distributed init (rank 1): env://
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.1+cuda11.6
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/8/-1->1->0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->4
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Trees [0] 10/4/-1->9->8 [1] 10/-1/-1->9->8
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->1
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/0/-1->4->-1
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 01 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 00 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 01 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/0 : 11[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 00/0 : 11[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/0 : 11[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 01/0 : 11[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 00 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 01 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01/0 : 8[1b0] -> 1[1c0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 00 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 01 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01/0 : 8[1b0] -> 1[1c0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01/0 : 1[1c0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01/0 : 1[1c0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 01 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO comm 0x5aabc6e0 rank 4 nranks 12 cudaDev 0 busId 1b0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO comm 0x5988f3e0 rank 6 nranks 12 cudaDev 2 busId 1d0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO comm 0x57fc8050 rank 5 nranks 12 cudaDev 1 busId 1c0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO comm 0x586e7e10 rank 7 nranks 12 cudaDev 3 busId 1e0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO comm 0x58a7dde0 rank 1 nranks 12 cudaDev 1 busId 1c0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO comm 0x587988f0 rank 0 nranks 12 cudaDev 0 busId 1b0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO comm 0x59e27fc0 rank 3 nranks 12 cudaDev 3 busId 1e0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO comm 0x58e55720 rank 2 nranks 12 cudaDev 2 busId 1d0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO comm 0x592ac1d0 rank 9 nranks 12 cudaDev 1 busId 1c0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO comm 0x59191dc0 rank 11 nranks 12 cudaDev 3 busId 1e0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO comm 0x59817290 rank 8 nranks 12 cudaDev 0 busId 1b0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO comm 0x5b9be300 rank 10 nranks 12 cudaDev 2 busId 1d0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
git:
  sha: 36cd6f63c4d40e1b7593c6fb2604a4016a0ae6ff, status: has uncommited changes, branch: main

arch: vit_small
batch_size_per_gpu: 30
center_crop: 0
channel_dict: {0: 'aTub', 1: 'BF', 2: 'DAPI', 3: 'Oct4', 4: 'PE'}
clip_grad: 3.0
dataset_dir: /fsx/fsx
dino_vit_name: mySCDINOmodel
dist_url: env://
drop_path_rate: 0.1
epochs: 3
freeze_last_layer: 1
full_ViT_name: mySCDINOmodel_
global_crops_scale: (0.4, 1.0)
gpu: 0
images_are_RGB: False
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
name_of_run: scDino_pcluster_full_0
norm_last_layer: True
norm_per_channel_file: /fsx/outputdir/scDino_pcluster_full_0/mean_and_std_of_dataset.txt
num_workers: 16
optimizer: adamw
out_dim: 65536
output_dir: /fsx/outputdir
parse_params: None
patch_size: 16
rank: 0
saveckp_freq: 1
seed: 40
selected_channels: [0, 1, 2, 3, 4]
teacher_temp: 0.04
train_datasetsplit_fraction: 0.8
upscale_factor: 0
use_bn_in_head: False
use_fp16: True
warmup_epochs: 1
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 12
saving log file of run parameters
ENTERING CUSTOM DATSET CLASS
Split between train and test dataset: 90852 22712
Train dataset consists of 90852 images.
Successfully loaded data.
Student and Teacher are built: they are both vit_small network.
Loss, optimizer and schedulers ready.
Starting DINO training !
Epoch: [0/3]  [  0/253]  eta: 0:45:26  loss: 10.830194 (10.830194)  lr: 0.000000 (0.000000)  wd: 0.040000 (0.040000)  time: 10.776649  data: 6.518417  max mem: 5693
Epoch: [0/3]  [ 10/253]  eta: 0:06:18  loss: 10.870738 (10.870789)  lr: 0.000014 (0.000014)  wd: 0.040039 (0.040054)  time: 1.556225  data: 0.592757  max mem: 6020
Epoch: [0/3]  [ 20/253]  eta: 0:04:20  loss: 10.879405 (10.881195)  lr: 0.000028 (0.000028)  wd: 0.040154 (0.040211)  time: 0.634089  data: 0.000188  max mem: 6020
Epoch: [0/3]  [ 30/253]  eta: 0:03:34  loss: 10.875791 (10.876825)  lr: 0.000056 (0.000042)  wd: 0.040616 (0.040470)  time: 0.637115  data: 0.000173  max mem: 6020
Epoch: [0/3]  [ 40/253]  eta: 0:03:08  loss: 10.854432 (10.866541)  lr: 0.000084 (0.000056)  wd: 0.041386 (0.040831)  time: 0.639413  data: 0.000182  max mem: 6020
Epoch: [0/3]  [ 50/253]  eta: 0:02:49  loss: 10.798764 (10.852881)  lr: 0.000112 (0.000070)  wd: 0.042461 (0.041295)  time: 0.639538  data: 0.000199  max mem: 6020
Epoch: [0/3]  [ 60/253]  eta: 0:02:35  loss: 10.768640 (10.833508)  lr: 0.000140 (0.000084)  wd: 0.043841 (0.041860)  time: 0.641402  data: 0.000188  max mem: 6020
Epoch: [0/3]  [ 70/253]  eta: 0:02:23  loss: 10.731096 (10.819501)  lr: 0.000167 (0.000098)  wd: 0.045522 (0.042526)  time: 0.642140  data: 0.000185  max mem: 6020
Epoch: [0/3]  [ 80/253]  eta: 0:02:12  loss: 10.715039 (10.800839)  lr: 0.000195 (0.000112)  wd: 0.047503 (0.043292)  time: 0.644038  data: 0.000185  max mem: 6020
Epoch: [0/3]  [ 90/253]  eta: 0:02:02  loss: 10.650015 (10.784176)  lr: 0.000223 (0.000126)  wd: 0.049778 (0.044157)  time: 0.647824  data: 0.000182  max mem: 6020
Epoch: [0/3]  [100/253]  eta: 0:01:53  loss: 10.626424 (10.769308)  lr: 0.000251 (0.000140)  wd: 0.052346 (0.045121)  time: 0.650324  data: 0.000197  max mem: 6020
Epoch: [0/3]  [110/253]  eta: 0:01:44  loss: 10.619548 (10.753735)  lr: 0.000279 (0.000153)  wd: 0.055200 (0.046182)  time: 0.651632  data: 0.000197  max mem: 6020
Epoch: [0/3]  [120/253]  eta: 0:01:36  loss: 10.579577 (10.739911)  lr: 0.000307 (0.000167)  wd: 0.058337 (0.047340)  time: 0.652462  data: 0.000196  max mem: 6020
Epoch: [0/3]  [130/253]  eta: 0:01:28  loss: 10.579238 (10.726494)  lr: 0.000335 (0.000181)  wd: 0.061751 (0.048593)  time: 0.654234  data: 0.000186  max mem: 6020
Epoch: [0/3]  [140/253]  eta: 0:01:21  loss: 10.550597 (10.712536)  lr: 0.000363 (0.000195)  wd: 0.065436 (0.049940)  time: 0.655352  data: 0.000169  max mem: 6020
Epoch: [0/3]  [150/253]  eta: 0:01:13  loss: 10.526325 (10.700911)  lr: 0.000391 (0.000209)  wd: 0.069385 (0.051380)  time: 0.655921  data: 0.000187  max mem: 6020
Epoch: [0/3]  [160/253]  eta: 0:01:06  loss: 10.526550 (10.690259)  lr: 0.000419 (0.000223)  wd: 0.073593 (0.052911)  time: 0.658096  data: 0.000199  max mem: 6020
Epoch: [0/3]  [170/253]  eta: 0:00:58  loss: 10.505713 (10.679021)  lr: 0.000446 (0.000237)  wd: 0.078051 (0.054531)  time: 0.660670  data: 0.000198  max mem: 6020
Epoch: [0/3]  [180/253]  eta: 0:00:51  loss: 10.495529 (10.668732)  lr: 0.000474 (0.000251)  wd: 0.082753 (0.056239)  time: 0.662335  data: 0.000185  max mem: 6020
Epoch: [0/3]  [190/253]  eta: 0:00:44  loss: 10.474164 (10.658689)  lr: 0.000502 (0.000265)  wd: 0.087689 (0.058033)  time: 0.665420  data: 0.000189  max mem: 6020

Node IP: 10.1.76.62
| distributed init (rank 0): env://
| distributed init (rank 6): env://
| distributed init (rank 5): env://
| distributed init (rank 15): env://
| distributed init (rank 11): env://
| distributed init (rank 7): env://
| distributed init (rank 13): env://
| distributed init (rank 12): env://
| distributed init (rank 14): env://
| distributed init (rank 10): env://
| distributed init (rank 9): env://
| distributed init (rank 8): env://
| distributed init (rank 4): env://
| distributed init (rank 3): env://
| distributed init (rank 2): env://
| distributed init (rank 1): env://
compute-gpu-dy-distributed-ml-1:11023:11023 [0] NCCL INFO Bootstrap : Using eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11023:11023 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:11023:11023 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:11023:11023 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.1+cuda11.6
compute-gpu-dy-distributed-ml-4:10802:10802 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10801:10801 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:10793:10793 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10801:10801 [2] NCCL INFO Bootstrap : Using eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10802:10802 [3] NCCL INFO Bootstrap : Using eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10801:10801 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10802:10802 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10802:10802 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-4:10801:10801 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:10794:10794 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:10814:10814 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:10793:10793 [2] NCCL INFO Bootstrap : Using eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-3:10794:10794 [3] NCCL INFO Bootstrap : Using eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-3:10793:10793 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:10793:10793 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:10794:10794 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:10794:10794 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:10814:10814 [3] NCCL INFO Bootstrap : Using eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-3:10792:10792 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:10814:10814 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:10814:10814 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:10811:10811 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:10791:10791 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:10792:10792 [1] NCCL INFO Bootstrap : Using eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-3:10792:10792 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:10792:10792 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:10811:10811 [0] NCCL INFO Bootstrap : Using eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-3:10791:10791 [0] NCCL INFO Bootstrap : Using eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-2:10811:10811 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:10811:10811 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:10791:10791 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:10791:10791 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:10812:10812 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:10813:10813 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:10812:10812 [1] NCCL INFO Bootstrap : Using eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-2:10812:10812 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:10812:10812 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:10813:10813 [2] NCCL INFO Bootstrap : Using eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-2:10813:10813 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:10813:10813 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:11024:11024 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:11024:11024 [1] NCCL INFO Bootstrap : Using eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11024:11024 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:11024:11024 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:11026:11026 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:11026:11026 [3] NCCL INFO Bootstrap : Using eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11026:11026 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:11026:11026 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:11025:11025 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:11025:11025 [2] NCCL INFO Bootstrap : Using eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11025:11025 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:11025:11025 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-4:10800:10800 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10800:10800 [1] NCCL INFO Bootstrap : Using eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10800:10800 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10800:10800 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-4:10799:10799 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10799:10799 [0] NCCL INFO Bootstrap : Using eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10799:10799 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10799:10799 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1

compute-gpu-dy-distributed-ml-3:10791:10852 [0] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-3:10792:10854 [1] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-3:10793:10851 [2] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-3:10791:10852 [0] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-3:10792:10854 [1] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-3:10793:10851 [2] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-3:10794:10853 [3] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-3:10794:10853 [3] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-4:10801:10861 [2] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-4:10799:10862 [0] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-4:10801:10861 [2] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-4:10799:10862 [0] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-4:10802:10863 [3] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-4:10802:10863 [3] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-4:10800:10860 [1] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-4:10800:10860 [1] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-2:10814:10876 [3] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-2:10814:10876 [3] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-2:10813:10877 [2] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-2:10813:10877 [2] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-2:10812:10878 [1] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-2:10812:10878 [1] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-2:10811:10879 [0] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-2:10811:10879 [0] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-1:11023:11090 [0] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-1:11023:11090 [0] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-1:11025:11092 [2] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-1:11026:11091 [3] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-1:11024:11089 [1] find_ofi_provider:753 NCCL WARN NET/OFI Couldn't find any optimal provider

compute-gpu-dy-distributed-ml-1:11025:11092 [2] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-1:11026:11091 [3] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

compute-gpu-dy-distributed-ml-1:11024:11089 [1] nccl_net_ofi_init:1415 NCCL WARN NET/OFI aws-ofi-nccl initialization failed
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO NET/Socket : Using [0]eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO NET/Socket : Using [0]eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO NET/Socket : Using [0]eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO NET/Socket : Using [0]eth0:10.1.125.84<0>
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO NET/Socket : Using [0]eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO NET/Socket : Using [0]eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO NET/Socket : Using [0]eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO NET/Socket : Using [0]eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO NET/Socket : Using [0]eth0:10.1.88.62<0>
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO NET/Socket : Using [0]eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO NET/Socket : Using [0]eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO NET/Socket : Using [0]eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO NET/Socket : Using [0]eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO NET/Socket : Using [0]eth0:10.1.76.62<0>
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO NET/IB : No device found.
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO NET/Socket : Using [0]eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO NET/Socket : Using [0]eth0:10.1.43.167<0>
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Using network Socket
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Trees [0] 9/12/-1->8->0 [1] 9/-1/-1->8->5
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Trees [0] 10/4/-1->9->8 [1] 10/-1/-1->9->8
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/4/-1->12->-1
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->4
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/8/-1->5->4
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/0/-1->4->12
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10799:10864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-3:10791:10857 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-1:11023:11094 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10811:10883 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Channel 00 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Channel 00 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Channel 01 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Channel 01 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 00/0 : 11[1e0] -> 12[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 00/0 : 15[1e0] -> 0[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Channel 00/0 : 11[1e0] -> 12[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Channel 00/0 : 15[1e0] -> 0[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10811:10883 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-3:10791:10857 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-4:10799:10864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-1:11023:11094 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 00 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 01 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 01/0 : 11[1e0] -> 12[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 00 : 12[1b0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 01 : 12[1b0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 01/0 : 15[1e0] -> 0[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:10792:10855 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Channel 01/0 : 11[1e0] -> 12[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Channel 01/0 : 15[1e0] -> 0[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Channel 00 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Channel 01 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Channel 00 : 13[1c0] -> 12[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Channel 01 : 13[1c0] -> 12[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Channel 00 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Channel 01 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Channel 01/0 : 5[1c0] -> 8[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-4:10799:10864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-3:10791:10857 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10811:10883 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 12[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 01/0 : 5[1c0] -> 8[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10812:10882 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Channel 01/0 : 8[1b0] -> 5[1c0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 12[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-4:10799:10864 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11023:11094 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 12[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10857 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10811:10883 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 01/0 : 12[1b0] -> 4[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 01/0 : 12[1b0] -> 4[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11023:11094 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 12[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10857 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Channel 00/0 : 12[1b0] -> 8[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10811:10883 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 00/0 : 12[1b0] -> 8[1b0] [receive] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Channel 00 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Channel 01 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Channel 01/0 : 8[1b0] -> 5[1c0] [send] via NET/Socket/0
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10799:10862 [0] NCCL INFO comm 0x59010610 rank 12 nranks 16 cudaDev 0 busId 1b0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-4:10800:10860 [1] NCCL INFO comm 0x59824dc0 rank 13 nranks 16 cudaDev 1 busId 1c0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-4:10801:10861 [2] NCCL INFO comm 0x5b1026f0 rank 14 nranks 16 cudaDev 2 busId 1d0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-4:10802:10863 [3] NCCL INFO comm 0x5b997400 rank 15 nranks 16 cudaDev 3 busId 1e0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:10793:10851 [2] NCCL INFO comm 0x59bffa20 rank 10 nranks 16 cudaDev 2 busId 1d0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:10791:10852 [0] NCCL INFO comm 0x59310bf0 rank 8 nranks 16 cudaDev 0 busId 1b0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:10794:10853 [3] NCCL INFO comm 0x59ea5040 rank 11 nranks 16 cudaDev 3 busId 1e0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:10792:10854 [1] NCCL INFO comm 0x5a053f60 rank 9 nranks 16 cudaDev 1 busId 1c0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:10812:10878 [1] NCCL INFO comm 0x59800950 rank 5 nranks 16 cudaDev 1 busId 1c0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:10814:10876 [3] NCCL INFO comm 0x5a41e140 rank 7 nranks 16 cudaDev 3 busId 1e0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:10813:10877 [2] NCCL INFO comm 0x58e1a2c0 rank 6 nranks 16 cudaDev 2 busId 1d0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:10811:10879 [0] NCCL INFO comm 0x596665a0 rank 4 nranks 16 cudaDev 0 busId 1b0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:11025:11092 [2] NCCL INFO comm 0x59d0cc40 rank 2 nranks 16 cudaDev 2 busId 1d0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:11024:11089 [1] NCCL INFO comm 0x59f175a0 rank 1 nranks 16 cudaDev 1 busId 1c0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:11023:11090 [0] NCCL INFO comm 0x58d8b800 rank 0 nranks 16 cudaDev 0 busId 1b0 commId 0xdd3e65564764bf77 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:11026:11091 [3] NCCL INFO comm 0x59c10d20 rank 3 nranks 16 cudaDev 3 busId 1e0 commId 0xdd3e65564764bf77 - Init COMPLETE
git:
  sha: 73a30cc47eb8d1ec6a648da265ac618a0fa23d2c, status: has uncommited changes, branch: main

arch: vit_small
batch_size_per_gpu: 30
center_crop: 0
channel_dict: {0: 'aTub', 1: 'BF', 2: 'DAPI', 3: 'Oct4', 4: 'PE'}
clip_grad: 3.0
dataset_dir: /fsx/fsx
dino_vit_name: mySCDINOmodel
dist_url: env://
drop_path_rate: 0.1
epochs: 3
freeze_last_layer: 1
full_ViT_name: mySCDINOmodel_
global_crops_scale: (0.4, 1.0)
gpu: 0
images_are_RGB: False
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
name_of_run: scDino_pcluster_full_0
norm_last_layer: True
norm_per_channel_file: /fsx/outputdir/scDino_pcluster_full_0/mean_and_std_of_dataset.txt
num_workers: 16
optimizer: adamw
out_dim: 65536
output_dir: /fsx/outputdir
parse_params: None
patch_size: 16
rank: 0
saveckp_freq: 1
seed: 40
selected_channels: [0, 1, 2, 3, 4]
teacher_temp: 0.04
train_datasetsplit_fraction: 0.8
upscale_factor: 0
use_bn_in_head: False
use_fp16: True
warmup_epochs: 1
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 16
saving log file of run parameters
ENTERING CUSTOM DATSET CLASS
Split between train and test dataset: 90852 22712
Train dataset consists of 90852 images.
Successfully loaded data.
Student and Teacher are built: they are both vit_small network.
Loss, optimizer and schedulers ready.
Starting DINO training !
Epoch: [0/3]  [  0/190]  eta: 0:38:52  loss: 10.821566 (10.821566)  lr: 0.000000 (0.000000)  wd: 0.040000 (0.040000)  time: 12.277204  data: 4.580762  max mem: 5693
Epoch: [0/3]  [ 10/190]  eta: 0:05:04  loss: 10.855537 (10.860128)  lr: 0.000025 (0.000025)  wd: 0.040068 (0.040096)  time: 1.688912  data: 0.416589  max mem: 6025
Epoch: [0/3]  [ 20/190]  eta: 0:03:21  loss: 10.869848 (10.873936)  lr: 0.000050 (0.000050)  wd: 0.040273 (0.040373)  time: 0.632202  data: 0.000184  max mem: 6025
Epoch: [0/3]  [ 30/190]  eta: 0:02:41  loss: 10.868000 (10.864987)  lr: 0.000099 (0.000074)  wd: 0.041092 (0.040833)  time: 0.634769  data: 0.000196  max mem: 6025
Epoch: [0/3]  [ 40/190]  eta: 0:02:17  loss: 10.818693 (10.848416)  lr: 0.000149 (0.000099)  wd: 0.042455 (0.041473)  time: 0.637858  data: 0.000198  max mem: 6025
Epoch: [0/3]  [ 50/190]  eta: 0:02:01  loss: 10.769097 (10.832212)  lr: 0.000198 (0.000124)  wd: 0.044357 (0.042292)  time: 0.642951  data: 0.000192  max mem: 6025
Epoch: [0/3]  [ 60/190]  eta: 0:01:47  loss: 10.744872 (10.810137)  lr: 0.000248 (0.000149)  wd: 0.046792 (0.043290)  time: 0.642912  data: 0.000193  max mem: 6025
Epoch: [0/3]  [ 70/190]  eta: 0:01:36  loss: 10.688664 (10.794648)  lr: 0.000298 (0.000174)  wd: 0.049753 (0.044464)  time: 0.640389  data: 0.000197  max mem: 6025
Epoch: [0/3]  [ 80/190]  eta: 0:01:26  loss: 10.650685 (10.773356)  lr: 0.000347 (0.000198)  wd: 0.053231 (0.045811)  time: 0.641357  data: 0.000191  max mem: 6025
Epoch: [0/3]  [ 90/190]  eta: 0:01:16  loss: 10.608396 (10.756926)  lr: 0.000397 (0.000223)  wd: 0.057216 (0.047331)  time: 0.643019  data: 0.000193  max mem: 6025
Epoch: [0/3]  [100/190]  eta: 0:01:07  loss: 10.608396 (10.741612)  lr: 0.000446 (0.000248)  wd: 0.061695 (0.049019)  time: 0.645634  data: 0.000190  max mem: 6025
Epoch: [0/3]  [110/190]  eta: 0:00:59  loss: 10.594767 (10.725387)  lr: 0.000496 (0.000273)  wd: 0.066655 (0.050873)  time: 0.646961  data: 0.000196  max mem: 6025
Epoch: [0/3]  [120/190]  eta: 0:00:51  loss: 10.548834 (10.710511)  lr: 0.000546 (0.000298)  wd: 0.072080 (0.052890)  time: 0.647753  data: 0.000193  max mem: 6025
Epoch: [0/3]  [130/190]  eta: 0:00:43  loss: 10.529326 (10.696707)  lr: 0.000595 (0.000322)  wd: 0.077955 (0.055066)  time: 0.650253  data: 0.000190  max mem: 6025
Epoch: [0/3]  [140/190]  eta: 0:00:36  loss: 10.525208 (10.684546)  lr: 0.000645 (0.000347)  wd: 0.084261 (0.057396)  time: 0.652186  data: 0.000194  max mem: 6025
Epoch: [0/3]  [150/190]  eta: 0:00:28  loss: 10.509398 (10.672436)  lr: 0.000694 (0.000372)  wd: 0.090979 (0.059877)  time: 0.651609  data: 0.000188  max mem: 6025
Epoch: [0/3]  [160/190]  eta: 0:00:21  loss: 10.503245 (10.662150)  lr: 0.000744 (0.000397)  wd: 0.098089 (0.062504)  time: 0.649694  data: 0.000171  max mem: 6025
Epoch: [0/3]  [170/190]  eta: 0:00:14  loss: 10.501752 (10.652483)  lr: 0.000794 (0.000422)  wd: 0.105570 (0.065273)  time: 0.647822  data: 0.000116  max mem: 6025
Epoch: [0/3]  [180/190]  eta: 0:00:07  loss: 10.490831 (10.642614)  lr: 0.000843 (0.000446)  wd: 0.113398 (0.068178)  time: 0.645993  data: 0.000077  max mem: 6025
Epoch: [0/3]  [189/190]  eta: 0:00:00  loss: 10.459015 (10.633210)  lr: 0.000888 (0.000469)  wd: 0.120720 (0.070905)  time: 0.635789  data: 0.000075  max mem: 6025
Epoch: [0/3] Total time: 0:02:14 (0.705761 s / it)
Averaged stats: loss: 10.459015 (10.633421)  lr: 0.000888 (0.000469)  wd: 0.120720 (0.070905)
Epoch: [1/3]  [  0/190]  eta: 0:15:23  loss: 10.445631 (10.445631)  lr: 0.000937 (0.000937)  wd: 0.130000 (0.130000)  time: 4.858028  data: 4.109158  max mem: 6026
Epoch: [1/3]  [ 10/190]  eta: 0:03:05  loss: 10.313362 (10.301057)  lr: 0.000937 (0.000937)  wd: 0.134329 (0.134342)  time: 1.031114  data: 0.373724  max mem: 6213
Epoch: [1/3]  [ 20/190]  eta: 0:02:24  loss: 10.171226 (10.209745)  lr: 0.000936 (0.000935)  wd: 0.138724 (0.138769)  time: 0.648259  data: 0.000191  max mem: 6213
Epoch: [1/3]  [ 30/190]  eta: 0:02:05  loss: 10.073186 (10.142871)  lr: 0.000930 (0.000933)  wd: 0.147695 (0.143274)  time: 0.647458  data: 0.000182  max mem: 6213
Epoch: [1/3]  [ 40/190]  eta: 0:01:52  loss: 9.949124 (10.083606)  lr: 0.000922 (0.000929)  wd: 0.156885 (0.147848)  time: 0.647891  data: 0.000178  max mem: 6213
Epoch: [1/3]  [ 50/190]  eta: 0:01:42  loss: 9.801307 (10.008715)  lr: 0.000911 (0.000924)  wd: 0.166267 (0.152487)  time: 0.648877  data: 0.000197  max mem: 6213
Epoch: [1/3]  [ 60/190]  eta: 0:01:33  loss: 9.663231 (9.943697)  lr: 0.000896 (0.000918)  wd: 0.175813 (0.157182)  time: 0.651490  data: 0.000202  max mem: 6213
Epoch: [1/3]  [ 70/190]  eta: 0:01:25  loss: 9.474236 (9.874112)  lr: 0.000879 (0.000912)  wd: 0.185492 (0.161926)  time: 0.656038  data: 0.000201  max mem: 6213
Epoch: [1/3]  [ 80/190]  eta: 0:01:17  loss: 9.450580 (9.819832)  lr: 0.000859 (0.000904)  wd: 0.195276 (0.166712)  time: 0.656544  data: 0.000206  max mem: 6213
Epoch: [1/3]  [ 90/190]  eta: 0:01:09  loss: 9.358305 (9.770800)  lr: 0.000836 (0.000895)  wd: 0.205136 (0.171533)  time: 0.653925  data: 0.000215  max mem: 6213
Epoch: [1/3]  [100/190]  eta: 0:01:02  loss: 9.273222 (9.703206)  lr: 0.000811 (0.000886)  wd: 0.215040 (0.176381)  time: 0.651563  data: 0.000200  max mem: 6213
Epoch: [1/3]  [110/190]  eta: 0:00:55  loss: 8.994946 (9.636560)  lr: 0.000784 (0.000875)  wd: 0.224960 (0.181248)  time: 0.653174  data: 0.000184  max mem: 6213
Epoch: [1/3]  [120/190]  eta: 0:00:48  loss: 8.949995 (9.580821)  lr: 0.000754 (0.000864)  wd: 0.234864 (0.186128)  time: 0.656068  data: 0.000182  max mem: 6213
Epoch: [1/3]  [130/190]  eta: 0:00:41  loss: 8.892660 (9.520908)  lr: 0.000722 (0.000852)  wd: 0.244724 (0.191012)  time: 0.654081  data: 0.000189  max mem: 6213
Epoch: [1/3]  [140/190]  eta: 0:00:34  loss: 8.815534 (9.468240)  lr: 0.000689 (0.000839)  wd: 0.254508 (0.195894)  time: 0.654203  data: 0.000189  max mem: 6213
Epoch: [1/3]  [150/190]  eta: 0:00:27  loss: 8.686370 (9.406687)  lr: 0.000654 (0.000826)  wd: 0.264187 (0.200765)  time: 0.655955  data: 0.000181  max mem: 6213
Epoch: [1/3]  [160/190]  eta: 0:00:20  loss: 8.468890 (9.346823)  lr: 0.000618 (0.000812)  wd: 0.273733 (0.205619)  time: 0.654843  data: 0.000181  max mem: 6213
Epoch: [1/3]  [170/190]  eta: 0:00:13  loss: 8.404229 (9.305519)  lr: 0.000580 (0.000798)  wd: 0.283115 (0.210447)  time: 0.654983  data: 0.000129  max mem: 6213
Epoch: [1/3]  [180/190]  eta: 0:00:06  loss: 8.377439 (9.248388)  lr: 0.000543 (0.000782)  wd: 0.292305 (0.215243)  time: 0.656021  data: 0.000079  max mem: 6213
Epoch: [1/3]  [189/190]  eta: 0:00:00  loss: 8.274563 (9.206295)  lr: 0.000508 (0.000769)  wd: 0.300390 (0.219526)  time: 0.636750  data: 0.000078  max mem: 6213
Epoch: [1/3] Total time: 0:02:08 (0.674686 s / it)
Averaged stats: loss: 8.274563 (9.205660)  lr: 0.000508 (0.000769)  wd: 0.300390 (0.219526)
Epoch: [2/3]  [  0/190]  eta: 0:10:37  loss: 8.095072 (8.095072)  lr: 0.000469 (0.000469)  wd: 0.310000 (0.310000)  time: 3.355811  data: 2.598872  max mem: 6213
Epoch: [2/3]  [ 10/190]  eta: 0:02:42  loss: 8.335346 (8.248127)  lr: 0.000450 (0.000450)  wd: 0.314261 (0.314247)  time: 0.903573  data: 0.236439  max mem: 6213
Epoch: [2/3]  [ 20/190]  eta: 0:02:14  loss: 8.235377 (8.146506)  lr: 0.000427 (0.000431)  wd: 0.318451 (0.318396)  time: 0.660134  data: 0.000198  max mem: 6213
Epoch: [2/3]  [ 30/190]  eta: 0:01:59  loss: 7.912940 (8.090721)  lr: 0.000388 (0.000411)  wd: 0.326602 (0.322441)  time: 0.663316  data: 0.000177  max mem: 6213
Epoch: [2/3]  [ 40/190]  eta: 0:01:49  loss: 8.025938 (8.123985)  lr: 0.000351 (0.000393)  wd: 0.334430 (0.326376)  time: 0.663801  data: 0.000174  max mem: 6213
Epoch: [2/3]  [ 50/190]  eta: 0:01:40  loss: 7.948799 (8.078846)  lr: 0.000314 (0.000374)  wd: 0.341911 (0.330195)  time: 0.663146  data: 0.000194  max mem: 6213
Epoch: [2/3]  [ 60/190]  eta: 0:01:31  loss: 7.843330 (8.062922)  lr: 0.000278 (0.000356)  wd: 0.349021 (0.333892)  time: 0.661751  data: 0.000190  max mem: 6213
Epoch: [2/3]  [ 70/190]  eta: 0:01:23  loss: 7.864337 (8.037840)  lr: 0.000243 (0.000338)  wd: 0.355739 (0.337463)  time: 0.661400  data: 0.000180  max mem: 6213
Epoch: [2/3]  [ 80/190]  eta: 0:01:16  loss: 7.861288 (8.008202)  lr: 0.000210 (0.000320)  wd: 0.362045 (0.340901)  time: 0.660757  data: 0.000178  max mem: 6213
Epoch: [2/3]  [ 90/190]  eta: 0:01:09  loss: 7.891092 (7.998018)  lr: 0.000179 (0.000303)  wd: 0.367920 (0.344202)  time: 0.660754  data: 0.000185  max mem: 6213
Epoch: [2/3]  [100/190]  eta: 0:01:01  loss: 7.960800 (7.988219)  lr: 0.000149 (0.000287)  wd: 0.373345 (0.347362)  time: 0.664102  data: 0.000193  max mem: 6213
Epoch: [2/3]  [110/190]  eta: 0:00:54  loss: 7.614172 (7.963450)  lr: 0.000122 (0.000271)  wd: 0.378305 (0.350375)  time: 0.664666  data: 0.000195  max mem: 6213
Epoch: [2/3]  [120/190]  eta: 0:00:47  loss: 7.666987 (7.944679)  lr: 0.000097 (0.000256)  wd: 0.382784 (0.353238)  time: 0.664192  data: 0.000188  max mem: 6213
Epoch: [2/3]  [130/190]  eta: 0:00:41  loss: 7.822084 (7.939531)  lr: 0.000075 (0.000241)  wd: 0.386769 (0.355947)  time: 0.666142  data: 0.000188  max mem: 6213
Epoch: [2/3]  [140/190]  eta: 0:00:34  loss: 7.973292 (7.944118)  lr: 0.000056 (0.000227)  wd: 0.390247 (0.358498)  time: 0.667627  data: 0.000195  max mem: 6213
Epoch: [2/3]  [150/190]  eta: 0:00:27  loss: 7.883542 (7.931905)  lr: 0.000039 (0.000214)  wd: 0.393208 (0.360888)  time: 0.667789  data: 0.000193  max mem: 6213
Epoch: [2/3]  [160/190]  eta: 0:00:20  loss: 7.677548 (7.924734)  lr: 0.000025 (0.000202)  wd: 0.395643 (0.363115)  time: 0.667820  data: 0.000168  max mem: 6213
Epoch: [2/3]  [170/190]  eta: 0:00:13  loss: 7.612685 (7.907923)  lr: 0.000014 (0.000191)  wd: 0.397545 (0.365175)  time: 0.667669  data: 0.000116  max mem: 6213
Epoch: [2/3]  [180/190]  eta: 0:00:06  loss: 7.847419 (7.908625)  lr: 0.000007 (0.000181)  wd: 0.398908 (0.367066)  time: 0.668039  data: 0.000080  max mem: 6213
Epoch: [2/3]  [189/190]  eta: 0:00:00  loss: 7.827360 (7.901382)  lr: 0.000003 (0.000172)  wd: 0.399669 (0.368622)  time: 0.648307  data: 0.000079  max mem: 6213
Epoch: [2/3] Total time: 0:02:08 (0.677978 s / it)
Averaged stats: loss: 7.827360 (7.924340)  lr: 0.000003 (0.000172)  wd: 0.399669 (0.368622)
compute-gpu-dy-distributed-ml-1:11025:11096 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-2:10814:10880 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-2:10811:10883 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-1:11024:11095 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-1:11026:11093 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-4:10800:10865 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-3:10793:10856 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-3:10794:10858 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-3:10792:10855 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-4:10802:10866 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-4:10799:10864 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-3:10791:10857 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-4:10801:10867 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-2:10812:10882 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-2:10813:10881 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-1:11025:11025 [2] NCCL INFO comm 0x59d0cc40 rank 2 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:10814:10814 [3] NCCL INFO comm 0x5a41e140 rank 7 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-1:11024:11024 [1] NCCL INFO comm 0x59f175a0 rank 1 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:10811:10811 [0] NCCL INFO comm 0x596665a0 rank 4 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-1:11026:11026 [3] NCCL INFO comm 0x59c10d20 rank 3 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:10793:10793 [2] NCCL INFO comm 0x59bffa20 rank 10 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10802:10802 [3] NCCL INFO comm 0x5b997400 rank 15 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10800:10800 [1] NCCL INFO comm 0x59824dc0 rank 13 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:10794:10794 [3] NCCL INFO comm 0x59ea5040 rank 11 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:10792:10792 [1] NCCL INFO comm 0x5a053f60 rank 9 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10801:10801 [2] NCCL INFO comm 0x5b1026f0 rank 14 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10799:10799 [0] NCCL INFO comm 0x59010610 rank 12 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:10791:10791 [0] NCCL INFO comm 0x59310bf0 rank 8 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:10813:10813 [2] NCCL INFO comm 0x58e1a2c0 rank 6 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:10812:10812 [1] NCCL INFO comm 0x59800950 rank 5 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
Training time 0:06:40
compute-gpu-dy-distributed-ml-1:11023:11094 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-1:11023:11023 [0] NCCL INFO comm 0x58d8b800 rank 0 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE

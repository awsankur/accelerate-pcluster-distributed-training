Node IP: 10.1.20.242
| distributed init (rank 0): env://
| distributed init (rank 2): env://
| distributed init (rank 3): env://
| distributed init (rank 4): env://
| distributed init (rank 10): env://
| distributed init (rank 5): env://
| distributed init (rank 7): env://
| distributed init (rank 9): env://
| distributed init (rank 6): env://
| distributed init (rank 8): env://
| distributed init (rank 11): env://
| distributed init (rank 1): env://
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.1+cuda11.6
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/8/-1->1->0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->4
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Trees [0] 10/4/-1->9->8 [1] 10/-1/-1->9->8
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->1
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/0/-1->4->-1
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 01 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 00 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 01 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/0 : 11[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 00/0 : 11[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/0 : 11[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 01/0 : 11[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 00 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Channel 01 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01/0 : 8[1b0] -> 1[1c0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 00 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Channel 01 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01/0 : 8[1b0] -> 1[1c0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01/0 : 1[1c0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 01/0 : 1[1c0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 00 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Channel 01 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:29165:29214 [0] NCCL INFO comm 0x5aabc6e0 rank 4 nranks 12 cudaDev 0 busId 1b0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:29167:29215 [2] NCCL INFO comm 0x5988f3e0 rank 6 nranks 12 cudaDev 2 busId 1d0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:29166:29213 [1] NCCL INFO comm 0x57fc8050 rank 5 nranks 12 cudaDev 1 busId 1c0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:29168:29216 [3] NCCL INFO comm 0x586e7e10 rank 7 nranks 12 cudaDev 3 busId 1e0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30395:30447 [1] NCCL INFO comm 0x58a7dde0 rank 1 nranks 12 cudaDev 1 busId 1c0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30394:30444 [0] NCCL INFO comm 0x587988f0 rank 0 nranks 12 cudaDev 0 busId 1b0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30397:30446 [3] NCCL INFO comm 0x59e27fc0 rank 3 nranks 12 cudaDev 3 busId 1e0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:30396:30445 [2] NCCL INFO comm 0x58e55720 rank 2 nranks 12 cudaDev 2 busId 1d0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22752:22803 [1] NCCL INFO comm 0x592ac1d0 rank 9 nranks 12 cudaDev 1 busId 1c0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22754:22802 [3] NCCL INFO comm 0x59191dc0 rank 11 nranks 12 cudaDev 3 busId 1e0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22751:22801 [0] NCCL INFO comm 0x59817290 rank 8 nranks 12 cudaDev 0 busId 1b0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:22753:22800 [2] NCCL INFO comm 0x5b9be300 rank 10 nranks 12 cudaDev 2 busId 1d0 commId 0x3b4388ba9f1cd1f9 - Init COMPLETE
git:
  sha: 36cd6f63c4d40e1b7593c6fb2604a4016a0ae6ff, status: has uncommited changes, branch: main

arch: vit_small
batch_size_per_gpu: 30
center_crop: 0
channel_dict: {0: 'aTub', 1: 'BF', 2: 'DAPI', 3: 'Oct4', 4: 'PE'}
clip_grad: 3.0
dataset_dir: /fsx/fsx
dino_vit_name: mySCDINOmodel
dist_url: env://
drop_path_rate: 0.1
epochs: 3
freeze_last_layer: 1
full_ViT_name: mySCDINOmodel_
global_crops_scale: (0.4, 1.0)
gpu: 0
images_are_RGB: False
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
name_of_run: scDino_pcluster_full_0
norm_last_layer: True
norm_per_channel_file: /fsx/outputdir/scDino_pcluster_full_0/mean_and_std_of_dataset.txt
num_workers: 16
optimizer: adamw
out_dim: 65536
output_dir: /fsx/outputdir
parse_params: None
patch_size: 16
rank: 0
saveckp_freq: 1
seed: 40
selected_channels: [0, 1, 2, 3, 4]
teacher_temp: 0.04
train_datasetsplit_fraction: 0.8
upscale_factor: 0
use_bn_in_head: False
use_fp16: True
warmup_epochs: 1
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 12
saving log file of run parameters
ENTERING CUSTOM DATSET CLASS
Split between train and test dataset: 90852 22712
Train dataset consists of 90852 images.
Successfully loaded data.
Student and Teacher are built: they are both vit_small network.
Loss, optimizer and schedulers ready.
Starting DINO training !
Epoch: [0/3]  [  0/253]  eta: 0:45:26  loss: 10.830194 (10.830194)  lr: 0.000000 (0.000000)  wd: 0.040000 (0.040000)  time: 10.776649  data: 6.518417  max mem: 5693
Epoch: [0/3]  [ 10/253]  eta: 0:06:18  loss: 10.870738 (10.870789)  lr: 0.000014 (0.000014)  wd: 0.040039 (0.040054)  time: 1.556225  data: 0.592757  max mem: 6020
Epoch: [0/3]  [ 20/253]  eta: 0:04:20  loss: 10.879405 (10.881195)  lr: 0.000028 (0.000028)  wd: 0.040154 (0.040211)  time: 0.634089  data: 0.000188  max mem: 6020
Epoch: [0/3]  [ 30/253]  eta: 0:03:34  loss: 10.875791 (10.876825)  lr: 0.000056 (0.000042)  wd: 0.040616 (0.040470)  time: 0.637115  data: 0.000173  max mem: 6020
Epoch: [0/3]  [ 40/253]  eta: 0:03:08  loss: 10.854432 (10.866541)  lr: 0.000084 (0.000056)  wd: 0.041386 (0.040831)  time: 0.639413  data: 0.000182  max mem: 6020
Epoch: [0/3]  [ 50/253]  eta: 0:02:49  loss: 10.798764 (10.852881)  lr: 0.000112 (0.000070)  wd: 0.042461 (0.041295)  time: 0.639538  data: 0.000199  max mem: 6020
Epoch: [0/3]  [ 60/253]  eta: 0:02:35  loss: 10.768640 (10.833508)  lr: 0.000140 (0.000084)  wd: 0.043841 (0.041860)  time: 0.641402  data: 0.000188  max mem: 6020
Epoch: [0/3]  [ 70/253]  eta: 0:02:23  loss: 10.731096 (10.819501)  lr: 0.000167 (0.000098)  wd: 0.045522 (0.042526)  time: 0.642140  data: 0.000185  max mem: 6020
Epoch: [0/3]  [ 80/253]  eta: 0:02:12  loss: 10.715039 (10.800839)  lr: 0.000195 (0.000112)  wd: 0.047503 (0.043292)  time: 0.644038  data: 0.000185  max mem: 6020
Epoch: [0/3]  [ 90/253]  eta: 0:02:02  loss: 10.650015 (10.784176)  lr: 0.000223 (0.000126)  wd: 0.049778 (0.044157)  time: 0.647824  data: 0.000182  max mem: 6020
Epoch: [0/3]  [100/253]  eta: 0:01:53  loss: 10.626424 (10.769308)  lr: 0.000251 (0.000140)  wd: 0.052346 (0.045121)  time: 0.650324  data: 0.000197  max mem: 6020
Epoch: [0/3]  [110/253]  eta: 0:01:44  loss: 10.619548 (10.753735)  lr: 0.000279 (0.000153)  wd: 0.055200 (0.046182)  time: 0.651632  data: 0.000197  max mem: 6020
Epoch: [0/3]  [120/253]  eta: 0:01:36  loss: 10.579577 (10.739911)  lr: 0.000307 (0.000167)  wd: 0.058337 (0.047340)  time: 0.652462  data: 0.000196  max mem: 6020
Epoch: [0/3]  [130/253]  eta: 0:01:28  loss: 10.579238 (10.726494)  lr: 0.000335 (0.000181)  wd: 0.061751 (0.048593)  time: 0.654234  data: 0.000186  max mem: 6020
Epoch: [0/3]  [140/253]  eta: 0:01:21  loss: 10.550597 (10.712536)  lr: 0.000363 (0.000195)  wd: 0.065436 (0.049940)  time: 0.655352  data: 0.000169  max mem: 6020
Epoch: [0/3]  [150/253]  eta: 0:01:13  loss: 10.526325 (10.700911)  lr: 0.000391 (0.000209)  wd: 0.069385 (0.051380)  time: 0.655921  data: 0.000187  max mem: 6020
Epoch: [0/3]  [160/253]  eta: 0:01:06  loss: 10.526550 (10.690259)  lr: 0.000419 (0.000223)  wd: 0.073593 (0.052911)  time: 0.658096  data: 0.000199  max mem: 6020
Epoch: [0/3]  [170/253]  eta: 0:00:58  loss: 10.505713 (10.679021)  lr: 0.000446 (0.000237)  wd: 0.078051 (0.054531)  time: 0.660670  data: 0.000198  max mem: 6020
Epoch: [0/3]  [180/253]  eta: 0:00:51  loss: 10.495529 (10.668732)  lr: 0.000474 (0.000251)  wd: 0.082753 (0.056239)  time: 0.662335  data: 0.000185  max mem: 6020
Epoch: [0/3]  [190/253]  eta: 0:00:44  loss: 10.474164 (10.658689)  lr: 0.000502 (0.000265)  wd: 0.087689 (0.058033)  time: 0.665420  data: 0.000189  max mem: 6020
Epoch: [0/3]  [200/253]  eta: 0:00:37  loss: 10.471665 (10.650088)  lr: 0.000530 (0.000279)  wd: 0.092853 (0.059912)  time: 0.669060  data: 0.000194  max mem: 6020
Epoch: [0/3]  [210/253]  eta: 0:00:30  loss: 10.481040 (10.641770)  lr: 0.000558 (0.000293)  wd: 0.098234 (0.061873)  time: 0.669015  data: 0.000180  max mem: 6020
Epoch: [0/3]  [220/253]  eta: 0:00:23  loss: 10.474449 (10.634373)  lr: 0.000586 (0.000307)  wd: 0.103823 (0.063915)  time: 0.668673  data: 0.000186  max mem: 6020
Epoch: [0/3]  [230/253]  eta: 0:00:16  loss: 10.457839 (10.626649)  lr: 0.000614 (0.000321)  wd: 0.109612 (0.066035)  time: 0.670029  data: 0.000139  max mem: 6020
Epoch: [0/3]  [240/253]  eta: 0:00:09  loss: 10.452947 (10.618975)  lr: 0.000642 (0.000335)  wd: 0.115590 (0.068231)  time: 0.671645  data: 0.000087  max mem: 6020
Epoch: [0/3]  [250/253]  eta: 0:00:02  loss: 10.433680 (10.611029)  lr: 0.000670 (0.000349)  wd: 0.121746 (0.070501)  time: 0.672083  data: 0.000087  max mem: 6020
Epoch: [0/3]  [252/253]  eta: 0:00:00  loss: 10.433680 (10.609547)  lr: 0.000675 (0.000352)  wd: 0.122998 (0.070964)  time: 0.664968  data: 0.000087  max mem: 6020
Epoch: [0/3] Total time: 0:02:55 (0.695380 s / it)
Averaged stats: loss: 10.433680 (10.607342)  lr: 0.000675 (0.000352)  wd: 0.122998 (0.070964)
Epoch: [1/3]  [  0/253]  eta: 0:20:58  loss: 10.412931 (10.412931)  lr: 0.000703 (0.000703)  wd: 0.130000 (0.130000)  time: 4.974328  data: 4.202774  max mem: 6028
Epoch: [1/3]  [ 10/253]  eta: 0:04:17  loss: 10.251722 (10.278640)  lr: 0.000703 (0.000703)  wd: 0.133245 (0.133253)  time: 1.060077  data: 0.382226  max mem: 6213
Epoch: [1/3]  [ 20/253]  eta: 0:03:23  loss: 10.198850 (10.180679)  lr: 0.000702 (0.000702)  wd: 0.136527 (0.136554)  time: 0.669467  data: 0.000175  max mem: 6213
Epoch: [1/3]  [ 30/253]  eta: 0:03:00  loss: 10.026694 (10.111653)  lr: 0.000700 (0.000701)  wd: 0.143198 (0.139900)  time: 0.671734  data: 0.000189  max mem: 6213
Epoch: [1/3]  [ 40/253]  eta: 0:02:45  loss: 9.885674 (10.045398)  lr: 0.000697 (0.000699)  wd: 0.150000 (0.143290)  time: 0.673356  data: 0.000195  max mem: 6213
Epoch: [1/3]  [ 50/253]  eta: 0:02:33  loss: 9.759730 (9.980861)  lr: 0.000692 (0.000697)  wd: 0.156922 (0.146719)  time: 0.673622  data: 0.000195  max mem: 6213
Epoch: [1/3]  [ 60/253]  eta: 0:02:23  loss: 9.644232 (9.914563)  lr: 0.000686 (0.000695)  wd: 0.163952 (0.150186)  time: 0.673965  data: 0.000209  max mem: 6213
Epoch: [1/3]  [ 70/253]  eta: 0:02:14  loss: 9.501731 (9.842498)  lr: 0.000678 (0.000692)  wd: 0.171078 (0.153686)  time: 0.673427  data: 0.000206  max mem: 6213
Epoch: [1/3]  [ 80/253]  eta: 0:02:05  loss: 9.338829 (9.772439)  lr: 0.000670 (0.000689)  wd: 0.178288 (0.157217)  time: 0.675199  data: 0.000203  max mem: 6213
Epoch: [1/3]  [ 90/253]  eta: 0:01:57  loss: 9.220443 (9.716935)  lr: 0.000660 (0.000685)  wd: 0.185569 (0.160776)  time: 0.674414  data: 0.000198  max mem: 6213
Epoch: [1/3]  [100/253]  eta: 0:01:49  loss: 9.119389 (9.653156)  lr: 0.000649 (0.000681)  wd: 0.192909 (0.164359)  time: 0.672271  data: 0.000183  max mem: 6213
Epoch: [1/3]  [110/253]  eta: 0:01:41  loss: 8.956540 (9.570450)  lr: 0.000636 (0.000676)  wd: 0.200296 (0.167964)  time: 0.673581  data: 0.000182  max mem: 6213
Epoch: [1/3]  [120/253]  eta: 0:01:34  loss: 8.735961 (9.502974)  lr: 0.000623 (0.000671)  wd: 0.207716 (0.171588)  time: 0.673337  data: 0.000186  max mem: 6213
Epoch: [1/3]  [130/253]  eta: 0:01:26  loss: 8.568217 (9.421729)  lr: 0.000609 (0.000666)  wd: 0.215158 (0.175226)  time: 0.673018  data: 0.000182  max mem: 6213
Epoch: [1/3]  [140/253]  eta: 0:01:19  loss: 8.187945 (9.332162)  lr: 0.000593 (0.000660)  wd: 0.222608 (0.178877)  time: 0.670634  data: 0.000176  max mem: 6213
Epoch: [1/3]  [150/253]  eta: 0:01:12  loss: 8.236731 (9.255901)  lr: 0.000577 (0.000654)  wd: 0.230053 (0.182537)  time: 0.668616  data: 0.000194  max mem: 6213
Epoch: [1/3]  [160/253]  eta: 0:01:05  loss: 8.269475 (9.196485)  lr: 0.000560 (0.000648)  wd: 0.237481 (0.186203)  time: 0.669681  data: 0.000202  max mem: 6213
Epoch: [1/3]  [170/253]  eta: 0:00:57  loss: 8.046119 (9.106332)  lr: 0.000542 (0.000641)  wd: 0.244879 (0.189871)  time: 0.669686  data: 0.000184  max mem: 6213
Epoch: [1/3]  [180/253]  eta: 0:00:50  loss: 7.905222 (9.049953)  lr: 0.000523 (0.000634)  wd: 0.252234 (0.193538)  time: 0.668778  data: 0.000175  max mem: 6213
Epoch: [1/3]  [190/253]  eta: 0:00:43  loss: 7.989148 (8.985320)  lr: 0.000504 (0.000627)  wd: 0.259535 (0.197202)  time: 0.668893  data: 0.000185  max mem: 6213
Epoch: [1/3]  [200/253]  eta: 0:00:36  loss: 7.776037 (8.931717)  lr: 0.000484 (0.000619)  wd: 0.266767 (0.200859)  time: 0.668326  data: 0.000186  max mem: 6213
Epoch: [1/3]  [210/253]  eta: 0:00:29  loss: 7.706923 (8.865779)  lr: 0.000463 (0.000612)  wd: 0.273920 (0.204506)  time: 0.667611  data: 0.000181  max mem: 6213
Epoch: [1/3]  [220/253]  eta: 0:00:22  loss: 7.569232 (8.807011)  lr: 0.000443 (0.000604)  wd: 0.280980 (0.208140)  time: 0.667601  data: 0.000199  max mem: 6213
Epoch: [1/3]  [230/253]  eta: 0:00:15  loss: 7.388472 (8.743260)  lr: 0.000421 (0.000595)  wd: 0.287935 (0.211758)  time: 0.663240  data: 0.000150  max mem: 6213
Epoch: [1/3]  [240/253]  eta: 0:00:08  loss: 7.345217 (8.690119)  lr: 0.000400 (0.000587)  wd: 0.294775 (0.215356)  time: 0.659767  data: 0.000082  max mem: 6213
Epoch: [1/3]  [250/253]  eta: 0:00:02  loss: 7.419959 (8.642348)  lr: 0.000378 (0.000578)  wd: 0.301486 (0.218932)  time: 0.661543  data: 0.000080  max mem: 6213
Epoch: [1/3]  [252/253]  eta: 0:00:00  loss: 7.419959 (8.627807)  lr: 0.000374 (0.000576)  wd: 0.302812 (0.219644)  time: 0.643523  data: 0.000079  max mem: 6213
Epoch: [1/3] Total time: 0:02:53 (0.686594 s / it)
Averaged stats: loss: 7.419959 (8.624608)  lr: 0.000374 (0.000576)  wd: 0.302812 (0.219644)
Epoch: [2/3]  [  0/253]  eta: 0:16:39  loss: 7.255652 (7.255652)  lr: 0.000352 (0.000352)  wd: 0.310000 (0.310000)  time: 3.948819  data: 3.211196  max mem: 6213
Epoch: [2/3]  [ 10/253]  eta: 0:03:57  loss: 7.440083 (7.480856)  lr: 0.000341 (0.000341)  wd: 0.313207 (0.313199)  time: 0.976266  data: 0.292121  max mem: 6213
Epoch: [2/3]  [ 20/253]  eta: 0:03:14  loss: 7.407064 (7.418129)  lr: 0.000328 (0.000330)  wd: 0.316373 (0.316343)  time: 0.678947  data: 0.000200  max mem: 6213
Epoch: [2/3]  [ 30/253]  eta: 0:02:54  loss: 7.190276 (7.335027)  lr: 0.000306 (0.000319)  wd: 0.322582 (0.319431)  time: 0.677551  data: 0.000182  max mem: 6213
Epoch: [2/3]  [ 40/253]  eta: 0:02:41  loss: 7.190276 (7.347940)  lr: 0.000285 (0.000309)  wd: 0.328614 (0.322459)  time: 0.681330  data: 0.000199  max mem: 6213
Epoch: [2/3]  [ 50/253]  eta: 0:02:30  loss: 7.383231 (7.402531)  lr: 0.000264 (0.000298)  wd: 0.334461 (0.325425)  time: 0.680044  data: 0.000209  max mem: 6213
Epoch: [2/3]  [ 60/253]  eta: 0:02:21  loss: 7.338953 (7.361165)  lr: 0.000243 (0.000287)  wd: 0.340111 (0.328326)  time: 0.672853  data: 0.000183  max mem: 6213
Epoch: [2/3]  [ 70/253]  eta: 0:02:12  loss: 7.108728 (7.314991)  lr: 0.000222 (0.000277)  wd: 0.345556 (0.331160)  time: 0.671826  data: 0.000176  max mem: 6213
Epoch: [2/3]  [ 80/253]  eta: 0:02:03  loss: 7.216599 (7.328657)  lr: 0.000202 (0.000267)  wd: 0.350785 (0.333925)  time: 0.670733  data: 0.000189  max mem: 6213
Epoch: [2/3]  [ 90/253]  eta: 0:01:55  loss: 7.248526 (7.295769)  lr: 0.000183 (0.000257)  wd: 0.355791 (0.336619)  time: 0.669456  data: 0.000182  max mem: 6213
Epoch: [2/3]  [100/253]  eta: 0:01:48  loss: 7.039469 (7.266608)  lr: 0.000164 (0.000247)  wd: 0.360564 (0.339238)  time: 0.673506  data: 0.000181  max mem: 6213
Epoch: [2/3]  [110/253]  eta: 0:01:40  loss: 7.019664 (7.255021)  lr: 0.000146 (0.000237)  wd: 0.365096 (0.341782)  time: 0.675848  data: 0.000192  max mem: 6213
Epoch: [2/3]  [120/253]  eta: 0:01:33  loss: 7.096776 (7.254525)  lr: 0.000129 (0.000227)  wd: 0.369379 (0.344248)  time: 0.671600  data: 0.000211  max mem: 6213
Epoch: [2/3]  [130/253]  eta: 0:01:26  loss: 7.182768 (7.232360)  lr: 0.000112 (0.000218)  wd: 0.373407 (0.346633)  time: 0.672700  data: 0.000217  max mem: 6213
Epoch: [2/3]  [140/253]  eta: 0:01:18  loss: 6.813609 (7.204125)  lr: 0.000097 (0.000209)  wd: 0.377172 (0.348937)  time: 0.671336  data: 0.000200  max mem: 6213
Epoch: [2/3]  [150/253]  eta: 0:01:11  loss: 6.813609 (7.181835)  lr: 0.000083 (0.000200)  wd: 0.380668 (0.351157)  time: 0.669097  data: 0.000197  max mem: 6213
Epoch: [2/3]  [160/253]  eta: 0:01:04  loss: 6.737472 (7.146185)  lr: 0.000069 (0.000192)  wd: 0.383888 (0.353292)  time: 0.671927  data: 0.000188  max mem: 6213
Epoch: [2/3]  [170/253]  eta: 0:00:57  loss: 6.686316 (7.143801)  lr: 0.000057 (0.000183)  wd: 0.386828 (0.355340)  time: 0.672034  data: 0.000190  max mem: 6213
Epoch: [2/3]  [180/253]  eta: 0:00:50  loss: 6.992186 (7.132518)  lr: 0.000046 (0.000176)  wd: 0.389482 (0.357299)  time: 0.670811  data: 0.000195  max mem: 6213
Epoch: [2/3]  [190/253]  eta: 0:00:43  loss: 7.005635 (7.128014)  lr: 0.000035 (0.000168)  wd: 0.391845 (0.359169)  time: 0.670227  data: 0.000197  max mem: 6213
Epoch: [2/3]  [200/253]  eta: 0:00:36  loss: 6.934813 (7.113188)  lr: 0.000027 (0.000161)  wd: 0.393915 (0.360947)  time: 0.669801  data: 0.000204  max mem: 6213
Epoch: [2/3]  [210/253]  eta: 0:00:29  loss: 6.658957 (7.089638)  lr: 0.000019 (0.000154)  wd: 0.395686 (0.362633)  time: 0.670631  data: 0.000202  max mem: 6213
Epoch: [2/3]  [220/253]  eta: 0:00:22  loss: 6.699847 (7.090349)  lr: 0.000013 (0.000147)  wd: 0.397157 (0.364226)  time: 0.673008  data: 0.000198  max mem: 6213
Epoch: [2/3]  [230/253]  eta: 0:00:15  loss: 7.129394 (7.082461)  lr: 0.000008 (0.000141)  wd: 0.398323 (0.365723)  time: 0.669783  data: 0.000139  max mem: 6213
Epoch: [2/3]  [240/253]  eta: 0:00:08  loss: 7.175694 (7.088475)  lr: 0.000004 (0.000136)  wd: 0.399185 (0.367125)  time: 0.664620  data: 0.000081  max mem: 6213
Epoch: [2/3]  [250/253]  eta: 0:00:02  loss: 7.175694 (7.088813)  lr: 0.000002 (0.000130)  wd: 0.399739 (0.368431)  time: 0.663287  data: 0.000081  max mem: 6213
Epoch: [2/3]  [252/253]  eta: 0:00:00  loss: 7.083926 (7.084379)  lr: 0.000002 (0.000129)  wd: 0.399813 (0.368681)  time: 0.644745  data: 0.000080  max mem: 6213
Epoch: [2/3] Total time: 0:02:53 (0.684749 s / it)
Averaged stats: loss: 7.083926 (7.030441)  lr: 0.000002 (0.000129)  wd: 0.399813 (0.368681)
compute-gpu-dy-distributed-ml-2:29168:29218 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-2:29166:29220 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-3:22754:22806 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-3:22752:22804 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-3:22751:22805 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-3:22753:22807 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-1:30396:30449 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-1:30395:30451 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-1:30397:30448 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-2:29165:29221 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-2:29167:29219 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-2:29166:29166 [1] NCCL INFO comm 0x57fc8050 rank 5 nranks 12 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-1:30396:30396 [2] NCCL INFO comm 0x58e55720 rank 2 nranks 12 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:29168:29168 [3] NCCL INFO comm 0x586e7e10 rank 7 nranks 12 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:22753:22753 [2] NCCL INFO comm 0x5b9be300 rank 10 nranks 12 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-1:30397:30397 [3] NCCL INFO comm 0x59e27fc0 rank 3 nranks 12 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:22754:22754 [3] NCCL INFO comm 0x59191dc0 rank 11 nranks 12 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-1:30395:30395 [1] NCCL INFO comm 0x58a7dde0 rank 1 nranks 12 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:22752:22752 [1] NCCL INFO comm 0x592ac1d0 rank 9 nranks 12 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:29167:29167 [2] NCCL INFO comm 0x5988f3e0 rank 6 nranks 12 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:22751:22751 [0] NCCL INFO comm 0x59817290 rank 8 nranks 12 cudaDev 0 busId 1b0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:29165:29165 [0] NCCL INFO comm 0x5aabc6e0 rank 4 nranks 12 cudaDev 0 busId 1b0 - Abort COMPLETE
Training time 0:08:52
compute-gpu-dy-distributed-ml-1:30394:30450 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-1:30394:30394 [0] NCCL INFO comm 0x587988f0 rank 0 nranks 12 cudaDev 0 busId 1b0 - Abort COMPLETE

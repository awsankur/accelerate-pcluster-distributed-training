Node IP: 10.1.20.242
| distributed init (rank 0): env://
| distributed init (rank 15): env://
| distributed init (rank 10): env://
| distributed init (rank 7): env://
| distributed init (rank 9): env://
| distributed init (rank 13): env://
| distributed init (rank 14): env://
| distributed init (rank 12): env://
| distributed init (rank 6): env://
| distributed init (rank 11): env://
| distributed init (rank 8): env://
| distributed init (rank 5): env://
| distributed init (rank 4): env://
| distributed init (rank 1): env://
| distributed init (rank 2): env://
| distributed init (rank 3): env://
compute-gpu-dy-distributed-ml-1:23590:23590 [0] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:23590:23590 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:23590:23590 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:23590:23590 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.1+cuda11.6
compute-gpu-dy-distributed-ml-1:23591:23591 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:23591:23591 [1] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:23591:23591 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:23591:23591 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:23592:23592 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:22583:22583 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:23592:23592 [2] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:23592:23592 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:23592:23592 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:22583:22583 [3] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:22583:22583 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:22583:22583 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:22580:22580 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:22580:22580 [0] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:22580:22580 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:22580:22580 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:22582:22582 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-1:23593:23593 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:22582:22582 [2] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:22582:22582 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:22582:22582 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:23593:23593 [3] NCCL INFO Bootstrap : Using eth0:10.1.20.242<0>
compute-gpu-dy-distributed-ml-1:23593:23593 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-1:23593:23593 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-2:22581:22581 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-2:22581:22581 [1] NCCL INFO Bootstrap : Using eth0:10.1.62.25<0>
compute-gpu-dy-distributed-ml-2:22581:22581 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-2:22581:22581 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:16175:16175 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:16175:16175 [1] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:16175:16175 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:16175:16175 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:16176:16176 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:16176:16176 [2] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:16176:16176 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:16176:16176 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:16174:16174 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:16174:16174 [0] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:16174:16174 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:16174:16174 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-3:16177:16177 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-3:16177:16177 [3] NCCL INFO Bootstrap : Using eth0:10.1.34.50<0>
compute-gpu-dy-distributed-ml-3:16177:16177 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-3:16177:16177 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-4:10798:10798 [0] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10798:10798 [0] NCCL INFO Bootstrap : Using eth0:10.1.88.131<0>
compute-gpu-dy-distributed-ml-4:10798:10798 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10798:10798 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-4:10799:10799 [1] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10799:10799 [1] NCCL INFO Bootstrap : Using eth0:10.1.88.131<0>
compute-gpu-dy-distributed-ml-4:10799:10799 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10799:10799 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-4:10801:10801 [3] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10801:10801 [3] NCCL INFO Bootstrap : Using eth0:10.1.88.131<0>
compute-gpu-dy-distributed-ml-4:10801:10801 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10801:10801 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-4:10800:10800 [2] NCCL INFO cudaDriverVersion 12020
compute-gpu-dy-distributed-ml-4:10800:10800 [2] NCCL INFO Bootstrap : Using eth0:10.1.88.131<0>
compute-gpu-dy-distributed-ml-4:10800:10800 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
compute-gpu-dy-distributed-ml-4:10800:10800 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.5.0aws
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/8/-1->5->4
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Trees [0] 9/12/-1->8->0 [1] 9/-1/-1->8->5
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Trees [0] 10/4/-1->9->8 [1] 10/-1/-1->9->8
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/4/-1->12->-1
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->4
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/0/-1->4->12
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Channel 00 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Channel 00 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Channel 01 : 10[1d0] -> 11[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Channel 01 : 9[1c0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 00/0 : 15[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 00/0 : 11[1e0] -> 12[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Channel 00/0 : 11[1e0] -> 12[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Channel 00/0 : 15[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Channel 00/0 : 7[1e0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 00 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 01 : 8[1b0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 01/0 : 15[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 01/0 : 11[1e0] -> 12[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Channel 01/0 : 15[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 00 : 12[1b0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Channel 01/0 : 11[1e0] -> 12[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 01 : 12[1b0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Channel 01/0 : 7[1e0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Channel 01/0 : 5[1c0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Channel 00 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Channel 01 : 10[1d0] -> 9[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Channel 00 : 13[1c0] -> 12[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Channel 01 : 13[1c0] -> 12[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Channel 00 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Channel 01 : 11[1e0] -> 10[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Connected all rings
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 01/0 : 5[1c0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 12[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Channel 01/0 : 8[1b0] -> 5[1c0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 12[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 9[1c0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 12[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 01/0 : 12[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 01/0 : 12[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 12[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 00/0 : 8[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Channel 00/0 : 12[1b0] -> 8[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 00/0 : 9[1c0] -> 4[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 00/0 : 12[1b0] -> 8[1b0] [receive] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Channel 00 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Channel 01 : 9[1c0] -> 8[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Channel 01/0 : 8[1b0] -> 5[1c0] [send] via NET/AWS Libfabric/0
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via SHM/direct/direct
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO Connected all trees
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-dy-distributed-ml-4:10798:10852 [0] NCCL INFO comm 0x589d59e0 rank 12 nranks 16 cudaDev 0 busId 1b0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-4:10800:10854 [2] NCCL INFO comm 0x5be86780 rank 14 nranks 16 cudaDev 2 busId 1d0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-4:10801:10855 [3] NCCL INFO comm 0x5991aef0 rank 15 nranks 16 cudaDev 3 busId 1e0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-4:10799:10853 [1] NCCL INFO comm 0x5959c070 rank 13 nranks 16 cudaDev 1 busId 1c0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:23590:23642 [0] NCCL INFO comm 0x591e6240 rank 0 nranks 16 cudaDev 0 busId 1b0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:23592:23644 [2] NCCL INFO comm 0x5b427a70 rank 2 nranks 16 cudaDev 2 busId 1d0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:23591:23643 [1] NCCL INFO comm 0x59782ed0 rank 1 nranks 16 cudaDev 1 busId 1c0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-1:23593:23645 [3] NCCL INFO comm 0x57e95c50 rank 3 nranks 16 cudaDev 3 busId 1e0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:16174:16226 [0] NCCL INFO comm 0x59c45510 rank 8 nranks 16 cudaDev 0 busId 1b0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:16175:16224 [1] NCCL INFO comm 0x5a7789b0 rank 9 nranks 16 cudaDev 1 busId 1c0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:16176:16225 [2] NCCL INFO comm 0x5a885bc0 rank 10 nranks 16 cudaDev 2 busId 1d0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-3:16177:16227 [3] NCCL INFO comm 0x592f5ff0 rank 11 nranks 16 cudaDev 3 busId 1e0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:22580:22629 [0] NCCL INFO comm 0x59283320 rank 4 nranks 16 cudaDev 0 busId 1b0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:22581:22632 [1] NCCL INFO comm 0x577e8980 rank 5 nranks 16 cudaDev 1 busId 1c0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:22582:22631 [2] NCCL INFO comm 0x5924d6b0 rank 6 nranks 16 cudaDev 2 busId 1d0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
compute-gpu-dy-distributed-ml-2:22583:22630 [3] NCCL INFO comm 0x5af6c210 rank 7 nranks 16 cudaDev 3 busId 1e0 commId 0x6eabe2cb2cfafe37 - Init COMPLETE
git:
  sha: 36cd6f63c4d40e1b7593c6fb2604a4016a0ae6ff, status: has uncommited changes, branch: main

arch: vit_small
batch_size_per_gpu: 30
center_crop: 0
channel_dict: {0: 'aTub', 1: 'BF', 2: 'DAPI', 3: 'Oct4', 4: 'PE'}
clip_grad: 3.0
dataset_dir: /fsx/fsx
dino_vit_name: mySCDINOmodel
dist_url: env://
drop_path_rate: 0.1
epochs: 3
freeze_last_layer: 1
full_ViT_name: mySCDINOmodel_
global_crops_scale: (0.4, 1.0)
gpu: 0
images_are_RGB: False
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
name_of_run: scDino_pcluster_full_0
norm_last_layer: True
norm_per_channel_file: /fsx/outputdir/scDino_pcluster_full_0/mean_and_std_of_dataset.txt
num_workers: 16
optimizer: adamw
out_dim: 65536
output_dir: /fsx/outputdir
parse_params: None
patch_size: 16
rank: 0
saveckp_freq: 1
seed: 40
selected_channels: [0, 1, 2, 3, 4]
teacher_temp: 0.04
train_datasetsplit_fraction: 0.8
upscale_factor: 0
use_bn_in_head: False
use_fp16: True
warmup_epochs: 1
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 16
saving log file of run parameters
ENTERING CUSTOM DATSET CLASS
Split between train and test dataset: 90852 22712
Train dataset consists of 90852 images.
Successfully loaded data.
Student and Teacher are built: they are both vit_small network.
Loss, optimizer and schedulers ready.
Starting DINO training !
Epoch: [0/3]  [  0/190]  eta: 0:36:15  loss: 10.821566 (10.821566)  lr: 0.000000 (0.000000)  wd: 0.040000 (0.040000)  time: 11.450832  data: 6.948110  max mem: 5693
Epoch: [0/3]  [ 10/190]  eta: 0:04:52  loss: 10.855547 (10.860128)  lr: 0.000025 (0.000025)  wd: 0.040068 (0.040096)  time: 1.622451  data: 0.631797  max mem: 6020
Epoch: [0/3]  [ 20/190]  eta: 0:03:16  loss: 10.869854 (10.873936)  lr: 0.000050 (0.000050)  wd: 0.040273 (0.040373)  time: 0.641208  data: 0.000177  max mem: 6020
Epoch: [0/3]  [ 30/190]  eta: 0:02:38  loss: 10.868005 (10.864988)  lr: 0.000099 (0.000074)  wd: 0.041092 (0.040833)  time: 0.643697  data: 0.000176  max mem: 6020
Epoch: [0/3]  [ 40/190]  eta: 0:02:16  loss: 10.818692 (10.848417)  lr: 0.000149 (0.000099)  wd: 0.042455 (0.041473)  time: 0.647187  data: 0.000180  max mem: 6020
Epoch: [0/3]  [ 50/190]  eta: 0:02:00  loss: 10.769087 (10.832213)  lr: 0.000198 (0.000124)  wd: 0.044357 (0.042292)  time: 0.650108  data: 0.000195  max mem: 6020
Epoch: [0/3]  [ 60/190]  eta: 0:01:47  loss: 10.744856 (10.810138)  lr: 0.000248 (0.000149)  wd: 0.046792 (0.043290)  time: 0.652922  data: 0.000191  max mem: 6020
Epoch: [0/3]  [ 70/190]  eta: 0:01:36  loss: 10.688676 (10.794650)  lr: 0.000298 (0.000174)  wd: 0.049753 (0.044464)  time: 0.656834  data: 0.000186  max mem: 6020
Epoch: [0/3]  [ 80/190]  eta: 0:01:26  loss: 10.650677 (10.773359)  lr: 0.000347 (0.000198)  wd: 0.053231 (0.045811)  time: 0.658034  data: 0.000199  max mem: 6020
Epoch: [0/3]  [ 90/190]  eta: 0:01:16  loss: 10.608418 (10.756930)  lr: 0.000397 (0.000223)  wd: 0.057216 (0.047331)  time: 0.659971  data: 0.000209  max mem: 6020
Epoch: [0/3]  [100/190]  eta: 0:01:08  loss: 10.608418 (10.741615)  lr: 0.000446 (0.000248)  wd: 0.061695 (0.049019)  time: 0.663340  data: 0.000201  max mem: 6020
Epoch: [0/3]  [110/190]  eta: 0:01:00  loss: 10.594762 (10.725389)  lr: 0.000496 (0.000273)  wd: 0.066655 (0.050873)  time: 0.664666  data: 0.000206  max mem: 6020
Epoch: [0/3]  [120/190]  eta: 0:00:52  loss: 10.548837 (10.710513)  lr: 0.000546 (0.000298)  wd: 0.072080 (0.052890)  time: 0.666079  data: 0.000221  max mem: 6020
Epoch: [0/3]  [130/190]  eta: 0:00:44  loss: 10.529319 (10.696709)  lr: 0.000595 (0.000322)  wd: 0.077955 (0.055066)  time: 0.668800  data: 0.000214  max mem: 6020
Epoch: [0/3]  [140/190]  eta: 0:00:36  loss: 10.525226 (10.684548)  lr: 0.000645 (0.000347)  wd: 0.084261 (0.057396)  time: 0.669868  data: 0.000194  max mem: 6020
Epoch: [0/3]  [150/190]  eta: 0:00:29  loss: 10.509406 (10.672439)  lr: 0.000694 (0.000372)  wd: 0.090979 (0.059877)  time: 0.671646  data: 0.000195  max mem: 6020
Epoch: [0/3]  [160/190]  eta: 0:00:21  loss: 10.503266 (10.662152)  lr: 0.000744 (0.000397)  wd: 0.098089 (0.062504)  time: 0.672083  data: 0.000193  max mem: 6020
Epoch: [0/3]  [170/190]  eta: 0:00:14  loss: 10.501746 (10.652486)  lr: 0.000794 (0.000422)  wd: 0.105570 (0.065273)  time: 0.668124  data: 0.000134  max mem: 6020
Epoch: [0/3]  [180/190]  eta: 0:00:07  loss: 10.490808 (10.642617)  lr: 0.000843 (0.000446)  wd: 0.113398 (0.068178)  time: 0.665971  data: 0.000082  max mem: 6020
Epoch: [0/3]  [189/190]  eta: 0:00:00  loss: 10.459011 (10.633213)  lr: 0.000888 (0.000469)  wd: 0.120720 (0.070905)  time: 0.654905  data: 0.000081  max mem: 6020
Epoch: [0/3] Total time: 0:02:16 (0.717141 s / it)
Averaged stats: loss: 10.459011 (10.633421)  lr: 0.000888 (0.000469)  wd: 0.120720 (0.070905)
Epoch: [1/3]  [  0/190]  eta: 0:15:58  loss: 10.445679 (10.445679)  lr: 0.000937 (0.000937)  wd: 0.130000 (0.130000)  time: 5.046690  data: 4.306996  max mem: 6026
Epoch: [1/3]  [ 10/190]  eta: 0:03:12  loss: 10.313363 (10.301090)  lr: 0.000937 (0.000937)  wd: 0.134329 (0.134342)  time: 1.071233  data: 0.391707  max mem: 6213
Epoch: [1/3]  [ 20/190]  eta: 0:02:30  loss: 10.171243 (10.209812)  lr: 0.000936 (0.000935)  wd: 0.138724 (0.138769)  time: 0.675738  data: 0.000187  max mem: 6213
Epoch: [1/3]  [ 30/190]  eta: 0:02:10  loss: 10.072783 (10.142872)  lr: 0.000930 (0.000933)  wd: 0.147695 (0.143274)  time: 0.677750  data: 0.000181  max mem: 6213
Epoch: [1/3]  [ 40/190]  eta: 0:01:57  loss: 9.949113 (10.083630)  lr: 0.000922 (0.000929)  wd: 0.156885 (0.147848)  time: 0.679067  data: 0.000181  max mem: 6213
Epoch: [1/3]  [ 50/190]  eta: 0:01:46  loss: 9.801409 (10.008755)  lr: 0.000911 (0.000924)  wd: 0.166267 (0.152487)  time: 0.681106  data: 0.000187  max mem: 6213
Epoch: [1/3]  [ 60/190]  eta: 0:01:37  loss: 9.663177 (9.943717)  lr: 0.000896 (0.000918)  wd: 0.175813 (0.157182)  time: 0.680891  data: 0.000186  max mem: 6213
Epoch: [1/3]  [ 70/190]  eta: 0:01:28  loss: 9.474470 (9.874064)  lr: 0.000879 (0.000912)  wd: 0.185492 (0.161926)  time: 0.681265  data: 0.000190  max mem: 6213
Epoch: [1/3]  [ 80/190]  eta: 0:01:20  loss: 9.450545 (9.819643)  lr: 0.000859 (0.000904)  wd: 0.195276 (0.166712)  time: 0.683049  data: 0.000183  max mem: 6213
Epoch: [1/3]  [ 90/190]  eta: 0:01:12  loss: 9.357046 (9.770597)  lr: 0.000836 (0.000895)  wd: 0.205136 (0.171533)  time: 0.685186  data: 0.000192  max mem: 6213
Epoch: [1/3]  [100/190]  eta: 0:01:05  loss: 9.273000 (9.703009)  lr: 0.000811 (0.000886)  wd: 0.215040 (0.176381)  time: 0.687241  data: 0.000206  max mem: 6213
Epoch: [1/3]  [110/190]  eta: 0:00:57  loss: 8.995819 (9.636390)  lr: 0.000784 (0.000875)  wd: 0.224960 (0.181248)  time: 0.688428  data: 0.000197  max mem: 6213
Epoch: [1/3]  [120/190]  eta: 0:00:50  loss: 8.949483 (9.580697)  lr: 0.000754 (0.000864)  wd: 0.234864 (0.186128)  time: 0.688583  data: 0.000188  max mem: 6213
Epoch: [1/3]  [130/190]  eta: 0:00:42  loss: 8.892965 (9.520799)  lr: 0.000722 (0.000852)  wd: 0.244724 (0.191012)  time: 0.688238  data: 0.000188  max mem: 6213
Epoch: [1/3]  [140/190]  eta: 0:00:35  loss: 8.815999 (9.468155)  lr: 0.000689 (0.000839)  wd: 0.254508 (0.195894)  time: 0.689478  data: 0.000201  max mem: 6213
Epoch: [1/3]  [150/190]  eta: 0:00:28  loss: 8.686713 (9.406600)  lr: 0.000654 (0.000826)  wd: 0.264187 (0.200765)  time: 0.688292  data: 0.000211  max mem: 6213
Epoch: [1/3]  [160/190]  eta: 0:00:21  loss: 8.469173 (9.346769)  lr: 0.000618 (0.000812)  wd: 0.273733 (0.205619)  time: 0.684592  data: 0.000182  max mem: 6213
Epoch: [1/3]  [170/190]  eta: 0:00:14  loss: 8.405087 (9.305484)  lr: 0.000580 (0.000798)  wd: 0.283115 (0.210447)  time: 0.678650  data: 0.000121  max mem: 6213
Epoch: [1/3]  [180/190]  eta: 0:00:07  loss: 8.376462 (9.248344)  lr: 0.000543 (0.000782)  wd: 0.292305 (0.215243)  time: 0.673969  data: 0.000081  max mem: 6213
Epoch: [1/3]  [189/190]  eta: 0:00:00  loss: 8.274276 (9.206268)  lr: 0.000508 (0.000769)  wd: 0.300390 (0.219526)  time: 0.652826  data: 0.000081  max mem: 6213
Epoch: [1/3] Total time: 0:02:13 (0.704356 s / it)
Averaged stats: loss: 8.274276 (9.205592)  lr: 0.000508 (0.000769)  wd: 0.300390 (0.219526)
Epoch: [2/3]  [  0/190]  eta: 0:12:08  loss: 8.095424 (8.095424)  lr: 0.000469 (0.000469)  wd: 0.310000 (0.310000)  time: 3.834976  data: 3.073878  max mem: 6213
Epoch: [2/3]  [ 10/190]  eta: 0:02:53  loss: 8.337616 (8.248363)  lr: 0.000450 (0.000450)  wd: 0.314261 (0.314247)  time: 0.963439  data: 0.279620  max mem: 6213
Epoch: [2/3]  [ 20/190]  eta: 0:02:20  loss: 8.235005 (8.146731)  lr: 0.000427 (0.000431)  wd: 0.318451 (0.318396)  time: 0.678217  data: 0.000181  max mem: 6213
Epoch: [2/3]  [ 30/190]  eta: 0:02:04  loss: 7.915370 (8.090879)  lr: 0.000388 (0.000411)  wd: 0.326602 (0.322441)  time: 0.680159  data: 0.000173  max mem: 6213
Epoch: [2/3]  [ 40/190]  eta: 0:01:53  loss: 8.025294 (8.124140)  lr: 0.000351 (0.000393)  wd: 0.334430 (0.326376)  time: 0.680755  data: 0.000188  max mem: 6213
Epoch: [2/3]  [ 50/190]  eta: 0:01:43  loss: 7.949231 (8.078998)  lr: 0.000314 (0.000374)  wd: 0.341911 (0.330195)  time: 0.682414  data: 0.000184  max mem: 6213
Epoch: [2/3]  [ 60/190]  eta: 0:01:35  loss: 7.842703 (8.063038)  lr: 0.000278 (0.000356)  wd: 0.349021 (0.333892)  time: 0.683754  data: 0.000187  max mem: 6213
Epoch: [2/3]  [ 70/190]  eta: 0:01:27  loss: 7.864335 (8.037858)  lr: 0.000243 (0.000338)  wd: 0.355739 (0.337463)  time: 0.683961  data: 0.000210  max mem: 6213
Epoch: [2/3]  [ 80/190]  eta: 0:01:19  loss: 7.859880 (8.008274)  lr: 0.000210 (0.000320)  wd: 0.362045 (0.340901)  time: 0.684705  data: 0.000205  max mem: 6213
Epoch: [2/3]  [ 90/190]  eta: 0:01:11  loss: 7.892570 (7.998080)  lr: 0.000179 (0.000303)  wd: 0.367920 (0.344202)  time: 0.685152  data: 0.000199  max mem: 6213
Epoch: [2/3]  [100/190]  eta: 0:01:04  loss: 7.961065 (7.988245)  lr: 0.000149 (0.000287)  wd: 0.373345 (0.347362)  time: 0.685531  data: 0.000192  max mem: 6213
Epoch: [2/3]  [110/190]  eta: 0:00:56  loss: 7.615345 (7.963490)  lr: 0.000122 (0.000271)  wd: 0.378305 (0.350375)  time: 0.685938  data: 0.000180  max mem: 6213
Epoch: [2/3]  [120/190]  eta: 0:00:49  loss: 7.667909 (7.944746)  lr: 0.000097 (0.000256)  wd: 0.382784 (0.353238)  time: 0.685949  data: 0.000183  max mem: 6213
Epoch: [2/3]  [130/190]  eta: 0:00:42  loss: 7.822060 (7.939654)  lr: 0.000075 (0.000241)  wd: 0.386769 (0.355947)  time: 0.686121  data: 0.000184  max mem: 6213
Epoch: [2/3]  [140/190]  eta: 0:00:35  loss: 7.973168 (7.944257)  lr: 0.000056 (0.000227)  wd: 0.390247 (0.358498)  time: 0.686464  data: 0.000185  max mem: 6213
Epoch: [2/3]  [150/190]  eta: 0:00:28  loss: 7.883835 (7.932021)  lr: 0.000039 (0.000214)  wd: 0.393208 (0.360888)  time: 0.687081  data: 0.000189  max mem: 6213
Epoch: [2/3]  [160/190]  eta: 0:00:21  loss: 7.677907 (7.924859)  lr: 0.000025 (0.000202)  wd: 0.395643 (0.363115)  time: 0.685146  data: 0.000184  max mem: 6213
Epoch: [2/3]  [170/190]  eta: 0:00:14  loss: 7.611984 (7.908018)  lr: 0.000014 (0.000191)  wd: 0.397545 (0.365175)  time: 0.675510  data: 0.000131  max mem: 6213
Epoch: [2/3]  [180/190]  eta: 0:00:06  loss: 7.847231 (7.908715)  lr: 0.000007 (0.000181)  wd: 0.398908 (0.367066)  time: 0.662475  data: 0.000083  max mem: 6213
Epoch: [2/3]  [189/190]  eta: 0:00:00  loss: 7.827685 (7.901482)  lr: 0.000003 (0.000172)  wd: 0.399669 (0.368622)  time: 0.636958  data: 0.000082  max mem: 6213
Epoch: [2/3] Total time: 0:02:12 (0.696206 s / it)
Averaged stats: loss: 7.827685 (7.924420)  lr: 0.000003 (0.000172)  wd: 0.399669 (0.368622)
compute-gpu-dy-distributed-ml-1:23593:23650 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-1:23592:23651 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-1:23591:23652 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-4:10800:10856 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-4:10798:10859 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-4:10801:10858 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-4:10799:10857 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-3:16176:16237 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-3:16177:16236 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-3:16175:16235 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-3:16174:16234 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-2:22580:22643 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-2:22581:22640 [1] NCCL INFO [Service thread] Connection closed by localRank 1
compute-gpu-dy-distributed-ml-2:22582:22642 [2] NCCL INFO [Service thread] Connection closed by localRank 2
compute-gpu-dy-distributed-ml-2:22583:22641 [3] NCCL INFO [Service thread] Connection closed by localRank 3
compute-gpu-dy-distributed-ml-1:23592:23592 [2] NCCL INFO comm 0x5b427a70 rank 2 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:16176:16176 [2] NCCL INFO comm 0x5a885bc0 rank 10 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10800:10800 [2] NCCL INFO comm 0x5be86780 rank 14 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:22582:22582 [2] NCCL INFO comm 0x5924d6b0 rank 6 nranks 16 cudaDev 2 busId 1d0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-1:23591:23591 [1] NCCL INFO comm 0x59782ed0 rank 1 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-1:23593:23593 [3] NCCL INFO comm 0x57e95c50 rank 3 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10799:10799 [1] NCCL INFO comm 0x5959c070 rank 13 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:22583:22583 [3] NCCL INFO comm 0x5af6c210 rank 7 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:16177:16177 [3] NCCL INFO comm 0x592f5ff0 rank 11 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:16175:16175 [1] NCCL INFO comm 0x5a7789b0 rank 9 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10801:10801 [3] NCCL INFO comm 0x5991aef0 rank 15 nranks 16 cudaDev 3 busId 1e0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:22581:22581 [1] NCCL INFO comm 0x577e8980 rank 5 nranks 16 cudaDev 1 busId 1c0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-4:10798:10798 [0] NCCL INFO comm 0x589d59e0 rank 12 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-2:22580:22580 [0] NCCL INFO comm 0x59283320 rank 4 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE
compute-gpu-dy-distributed-ml-3:16174:16174 [0] NCCL INFO comm 0x59c45510 rank 8 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE
Training time 0:06:51
compute-gpu-dy-distributed-ml-1:23590:23653 [0] NCCL INFO [Service thread] Connection closed by localRank 0
compute-gpu-dy-distributed-ml-1:23590:23590 [0] NCCL INFO comm 0x591e6240 rank 0 nranks 16 cudaDev 0 busId 1b0 - Abort COMPLETE
